# NodPT Executor

Background worker service built with .NET 8 that processes jobs from Redis streams and executes AI-powered tasks. The Executor is the core processing engine that orchestrates workflow execution and AI interactions.

## ğŸ› ï¸ Technology Stack

- **.NET 8.0**: Modern .NET framework for background services
- **Worker Service**: Long-running background service template
- **Redis Streams**: Job queue and message streaming
- **StackExchange.Redis**: Redis client library
- **HTTP Client**: Communication with AI services (Ollama)
- **System.Text.Json**: JSON serialization

### Key Features

- Role-based job execution (Manager, Inspector, Agent)
- Concurrent job processing with configurable limits
- Redis Streams consumer groups
- LLM chat integration
- SignalR notifications (via Redis)
- Docker ready

## ğŸ—ï¸ Architecture

### Job Processing Flow

```
Redis Stream (jobs:manager/inspector/agent)
    â”‚
    â–¼
Executor Consumer
    â”‚
    â”œâ”€â†’ Manager Runner â”€â”€â†’ LLM (trt-llm-manager)
    â”œâ”€â†’ Inspector Runner â”€â”€â†’ LLM (trt-llm-inspector)
    â””â”€â†’ Agent Runner â”€â”€â†’ LLM (trt-llm-agent)
    â”‚
    â–¼
Process Result
    â”‚
    â”œâ”€â†’ Save to Repository
    â””â”€â†’ Notify via SignalR (Redis stream: signalr:updates)
```

### Project Structure

```
Executor/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ Config/            # Configuration classes
â”‚   â”œâ”€â”€ Consumers/         # Redis Streams consumers
â”‚   â”‚   â”œâ”€â”€ JobConsumer.cs        # Job stream consumer
â”‚   â”‚   â””â”€â”€ ChatConsumer.cs       # Chat stream consumer
â”‚   â”œâ”€â”€ Data/              # Data structures and interfaces
â”‚   â”‚   â”œâ”€â”€ IJobRepository.cs     # Repository interface
â”‚   â”‚   â””â”€â”€ JobMessage.cs         # Job data model
â”‚   â”œâ”€â”€ Dispatch/          # Job dispatcher
â”‚   â”‚   â””â”€â”€ JobDispatcher.cs      # Concurrency control
â”‚   â”œâ”€â”€ Notify/            # Notification interfaces
â”‚   â”‚   â””â”€â”€ ISignalRNotifier.cs   # SignalR notification
â”‚   â”œâ”€â”€ Runners/           # Job execution runners
â”‚   â”‚   â”œâ”€â”€ ManagerRunner.cs      # Manager job runner
â”‚   â”‚   â”œâ”€â”€ InspectorRunner.cs    # Inspector job runner
â”‚   â”‚   â””â”€â”€ AgentRunner.cs        # Agent job runner
â”‚   â”œâ”€â”€ Services/          # External services
â”‚   â”‚   â””â”€â”€ LlmChatService.cs     # LLM communication
â”‚   â”œâ”€â”€ Program.cs         # Application entry point
â”‚   â”œâ”€â”€ Worker.cs          # Background worker for jobs
â”‚   â”œâ”€â”€ ChatWorker.cs      # Background worker for chat
â”‚   â””â”€â”€ BackendExecutor.csproj    # Project file
â”œâ”€â”€ Dockerfile             # Docker container config
â”œâ”€â”€ docker-compose.yml     # Docker Compose config
â””â”€â”€ README.md             # This file
```

## ğŸš€ Getting Started

### Prerequisites

- .NET 8.0 SDK or later
- Redis server (with Streams support)
- AI service (Ollama or compatible LLM endpoint)
- Docker (for containerized deployment)

### Local Development

1. **Install .NET 8 SDK**:
   Download from [dotnet.microsoft.com](https://dotnet.microsoft.com/download)

2. **Navigate to project**:
   ```bash
   cd Executor/src
   ```

3. **Restore dependencies**:
   ```bash
   dotnet restore
   ```

4. **Configure appsettings.Development.json**:
   ```json
   {
     "Redis": {
       "ConnectionString": "localhost:6379"
     },
     "Concurrency": {
       "MaxManager": 5,
       "MaxInspector": 10,
       "MaxAgent": 20,
       "MaxTotal": 50
     },
     "LLM": {
       "Endpoint": "http://localhost:11434/v1/chat/completions"
     }
   }
   ```

5. **Run the application**:
   ```bash
   dotnet run
   ```

### Build for Production

```bash
dotnet build -c Release
dotnet publish -c Release -o ./publish
```

## ğŸ³ Docker Deployment

### Environment Setup

Create or update environment file at `/home/runner_user/envs/backend.env`:

```env
# Redis Configuration
REDIS_CONNECTION=nodpt-redis:6379

# Concurrency Limits (0 = unlimited)
MAX_MANAGER=5
MAX_INSPECTOR=10
MAX_AGENT=20
MAX_TOTAL=50

# LLM Configuration
LLM_ENDPOINT=http://ollama:11434/v1/chat/completions

# ASP.NET Core Configuration
ASPNETCORE_ENVIRONMENT=Production
```

### Build and Run with Docker

```bash
# Create network if not exists
docker network create backend_network

# Build the image (from repository root)
cd Executor
docker-compose build

# Start the container
docker-compose up -d

# View logs
docker-compose logs -f

# Stop the container
docker-compose down
```

### Dockerfile

Multi-stage build optimized for .NET 8:

1. **Build Stage**: Builds the application
2. **Publish Stage**: Creates release package
3. **Runtime Stage**: Uses .NET runtime (lightweight)

## âš™ï¸ Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `REDIS_CONNECTION` | `localhost:6379` | Redis connection string |
| `MAX_MANAGER` | `0` | Max concurrent manager jobs (0 = unlimited) |
| `MAX_INSPECTOR` | `0` | Max concurrent inspector jobs (0 = unlimited) |
| `MAX_AGENT` | `0` | Max concurrent agent jobs (0 = unlimited) |
| `MAX_TOTAL` | `0` | Max total concurrent jobs (0 = unlimited) |
| `LLM_ENDPOINT` | `http://localhost:11434/v1/chat/completions` | LLM API endpoint |

### Concurrency Control

The dispatcher ensures jobs don't exceed configured limits:

- Per-role limits (Manager, Inspector, Agent)
- Total concurrent job limit
- Automatic queuing when at capacity

## ğŸ“¨ Job Message Format

Jobs are added to Redis Streams with the following format:

```json
{
  "jobId": "unique-job-id",
  "workflowId": "workflow-id",
  "userId": "user-id",
  "projectId": "project-id",
  "connectionId": "signalr-connection-id",
  "task": "task-description",
  "payload": "{\"key\":\"value\"}"
}
```

### Redis Stream Keys

- `jobs:manager`: Manager-level jobs (high-level planning)
- `jobs:inspector`: Inspector-level jobs (code review, analysis)
- `jobs:agent`: Agent-level jobs (specific tasks)
- `signalr:updates`: Results sent to SignalR for frontend delivery

## ğŸ¤– LLM Integration

### LLM Chat Service

The executor includes an `LlmChatService` for AI interactions.

#### Send String Message

```csharp
private readonly ILlmChatService _llmChatService;

var response = await _llmChatService.SendChatMessageAsync(
    message: "Explain this code",
    model: "trt-llm-manager",
    maxTokens: 128,
    cancellationToken: cancellationToken
);
```

#### Send Object Message

```csharp
var messageObject = new
{
    prompt = "Analyze this workflow",
    context = "Node-based editor",
    requirements = new[] { "performance", "security" }
};

var response = await _llmChatService.SendChatMessageAsync(
    messageObject: messageObject,
    model: "trt-llm-inspector",
    maxTokens: 256,
    cancellationToken: cancellationToken
);
```

### LLM API Format

Request format (OpenAI-compatible):

```json
{
  "model": "model-name",
  "messages": [
    {
      "role": "user",
      "content": "message-content"
    }
  ],
  "max_tokens": 128
}
```

Response format:

```json
{
  "choices": [
    {
      "message": {
        "content": "AI response text"
      }
    }
  ]
}
```

### Supported Models

- `trt-llm-manager`: Manager-level reasoning
- `trt-llm-inspector`: Code inspection and analysis
- `trt-llm-agent`: Task execution

## ğŸ”„ Job Execution Flow

1. **Consumer reads from Redis Stream**: `jobs:{role}` stream
2. **Dispatcher checks concurrency**: Ensure limits not exceeded
3. **Runner processes job**:
   - Extract job data
   - Call LLM service if needed
   - Process results
4. **Save results**: Store in repository (database)
5. **Notify frontend**: Send results to `signalr:updates` stream
6. **Acknowledge job**: Mark as processed in Redis

### Consumer Groups

Redis consumer groups ensure reliable processing:

- Group name: `executor-group`
- Consumer name: Unique per instance
- Auto-creation of groups on startup

## ğŸ“Š Job Runners

### Manager Runner

High-level planning and orchestration:

```csharp
public async Task<string> ExecuteAsync(JobMessage job, CancellationToken ct)
{
    // Use LLM for planning
    var response = await _llmService.SendChatMessageAsync(
        message: job.Task,
        model: "trt-llm-manager",
        maxTokens: 256,
        cancellationToken: ct
    );
    
    return response;
}
```

### Inspector Runner

Code review and analysis:

```csharp
public async Task<string> ExecuteAsync(JobMessage job, CancellationToken ct)
{
    // Use LLM for inspection
    var response = await _llmService.SendChatMessageAsync(
        messageObject: new { code = job.Payload, task = job.Task },
        model: "trt-llm-inspector",
        maxTokens: 512,
        cancellationToken: ct
    );
    
    return response;
}
```

### Agent Runner

Specific task execution:

```csharp
public async Task<string> ExecuteAsync(JobMessage job, CancellationToken ct)
{
    // Use LLM for task execution
    var response = await _llmService.SendChatMessageAsync(
        message: job.Task,
        model: "trt-llm-agent",
        maxTokens: 128,
        cancellationToken: ct
    );
    
    return response;
}
```

## ğŸ”” SignalR Notifications

Results are sent to SignalR via Redis stream:

```csharp
// Write to signalr:updates stream
await _redis.GetDatabase().StreamAddAsync("signalr:updates", new[]
{
    new NameValueEntry("MessageId", messageId),
    new NameValueEntry("NodeId", nodeId),
    new NameValueEntry("ProjectId", projectId),
    new NameValueEntry("UserId", userId),
    new NameValueEntry("Type", "result"),
    new NameValueEntry("Payload", result),
    new NameValueEntry("Timestamp", DateTime.UtcNow.ToString("o"))
});
```

SignalR service reads this stream and delivers to connected clients.

## ğŸ§ª Testing

```bash
# Run tests (if available)
dotnet test

# Test Redis connection
redis-cli -h localhost -p 6379 ping

# Test LLM endpoint
curl -X POST http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"llama2","messages":[{"role":"user","content":"test"}]}'
```

## ğŸ¤ Contributing

### Development Guidelines

1. Use async/await for all I/O operations
2. Implement proper cancellation token handling
3. Use structured logging with proper log levels
4. Handle exceptions gracefully
5. Write unit tests for runners and services
6. Document complex logic with comments

### Adding New Job Runners

1. Create runner class implementing `IJobRunner`
2. Register in dependency injection (Program.cs)
3. Add corresponding Redis stream
4. Configure concurrency limits
5. Update documentation

### Code Style

- Follow .NET naming conventions
- Use dependency injection
- Keep runners focused and single-purpose
- Use configuration for external dependencies

## ğŸ› Troubleshooting

### Common Issues

**Redis connection fails**:
```bash
# Verify Redis is running
docker ps | grep redis

# Test connection
redis-cli -h nodpt-redis -p 6379 ping
```

**LLM endpoint not responding**:
```bash
# Check Ollama service
docker ps | grep ollama

# Test endpoint
curl http://ollama:11434/api/tags
```

**Jobs not being processed**:
- Check Redis streams: `redis-cli XINFO STREAM jobs:manager`
- Verify consumer group exists
- Check executor logs: `docker-compose logs -f`
- Ensure concurrency limits not too restrictive

**Out of memory errors**:
- Reduce concurrency limits
- Check LLM max_tokens settings
- Monitor container resource usage

## ğŸ“ˆ Monitoring

### Logs

```bash
# Docker logs
docker-compose logs -f nodpt-executor

# .NET logs location (if running locally)
./logs/executor-{date}.log
```

### Metrics to Monitor

- Job processing rate
- Job queue depth (Redis stream length)
- Concurrent job count
- LLM response times
- Error rates
- Memory usage

## ğŸ”’ Security

- Never log sensitive job payloads
- Validate all job data before processing
- Use secure connections to Redis and LLM
- Implement rate limiting if exposed publicly
- Regular security updates for dependencies

## ğŸ“š Dependencies

This project depends on:
- **NodPT.Data**: Shared data layer (optional, currently stubbed)
- **Redis**: Message streaming and job queuing
- **AI Service**: LLM endpoint (Ollama or compatible)

## ğŸ“ Support

For issues and questions:
- Open an issue on GitHub
- Check executor logs for errors
- Verify Redis and AI services are running
- Contact the development team